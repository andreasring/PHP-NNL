---------------------------------------------------------------------
TODO / Review
---------------------------------------------------------------------
- Bias neuron
  - Currently there is one bias neuron for each and every layer. But do we need this? Should be sufficient to just have one bias neuron, for example in the input layer and connect that to all the other neurons in the network?


- Activation function
  - Possible other functions to implement
    - Gaussian
    - Linear
    - Step (binary)


- Normalization
  - Create normalization helper functions


- Make weights into objects
  - This makes it easy to access them from some centralized place
- Make all objects avalible through centralized stores?


- Learning rate
  - We should have a global setLearningRate() that sets a learning rate for all neurons AND create a setLearningRate for each neuron to do custom learning rates for each neuron

- Momentum
  - Currently not implemented

- Save functions should be filtered a bit. Meaning, we don't need to save training and control data etc.

- Making a nicer set of API functions for generating a totally custom network: That is, you do not send the structure of the network to the contrusctor but you do things like addLayer(), addNeuron(), addSynapse and so on. These functions should be the ones used by the more automated construct API too.

- Create some sort of mapping/query function so you can get exactly what synapse goes between two neurons

- A way to "record" everything the network does and replay it in the browser. Some sort of snapshot function. Maybe we can just use the save/load/export functions and add a bit of meta data to it? Sequence number, ID or timestamp.

- Could we maybe do this all in database format? Sqlite or mysql with memory tables? Only requirement is that the performance has to be acceptable.

---------------------------------------------------------------------
Random notes
---------------------------------------------------------------------
Training algo / Backpropagation algo
  - Gradient decent
  - Genetic

Full-batch training vs Mini-batch training vs Online training

Autoencoder


Treshold function in neuron before activation function!?

Overfitting
  - Dropout
  - Weight decay
  - Weight sharing
  - Early stopping
  - Model averaging
  - Bayesian fitting of neural nets
  - Generative pre-training


Error calculation
  - Mean squared error (MSE)
  - Root mean squered error (RMS/RMSE)
  - Arctan

Cost function?
  - Softmax
Cross-entropy





Network architecture types
  - Feed farward network
  - Recurrent neural network
  - Symetrically connected network

Unlabeled/unsupervised/feature extraction/pattern recognition
  - Restricted boltzman machine
  - Autoencoders

Labeled/supervised/classifier
  - Reccurent network
  - Recursive neural tenser network
  - Convolutional network
  - Deep belif network


Reinforcement learning?
