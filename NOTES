---------------------------------------------------------------------
TODO / Review
---------------------------------------------------------------------

- Normalization
  - Create normalization helper functions

- Make weights into objects
  - This makes it easy to access them from some centralized place

- Learning rate
  - We should have a global setLearningRate() that sets a learning rate for all neurons AND create a setLearningRate for each neuron to do custom learning rates for each neuron

- Momentum
  - Currently not implemented -> Option for gradient decent

- Making a nicer set of API functions for generating a totally custom network: That is, you do not send the structure of the network to the contrusctor but you do things like addLayer(), addNeuron(), addSynapse and so on. These functions should be the ones used by the more automated construct API too.

- Create some sort of mapping/query function so you can get exactly what synapse goes between two neurons

- A way to "record" everything the network does and replay it in the browser. Some sort of snapshot function. Maybe we can just use the save/load/export functions and add a bit of meta data to it? Sequence number, ID or timestamp.

- Could we maybe do this all in database format? Sqlite or mysql with memory tables? Only requirement is that the performance has to be acceptable.

- Create a loss and or cost function namespace?
  - Should be easy enough to just supply the functions with expected output and actual output and then do whatever they want with that


---------------------------------------------------------------------
Random notes
---------------------------------------------------------------------

Full-batch training vs Mini-batch training vs Online training
  - Stochastic = online

Autoencoders
  - Denoising autoencoder
  - Sparse autoencoder
  - Variational autoencoder (VAE)
  - Contractive autoencoder

Overfitting
  - Dropout
  - Weight decay
  - Weight sharing
  - Early stopping
  - Model averaging
  - Bayesian fitting of neural nets
  - Generative pre-training
  - Gradient noise
  - Other types of noise

Error calculation
  - Mean squared error (MSE)
  - Root mean squered error (RMS/RMSE)
  - Arctan


Softmax

Maxout

Cross-entropy

Outlier removal

Stochastic gradient descent (https://en.wikipedia.org/wiki/Stochastic_gradient_descent)

Gradient decent additions
  - Momentum
  - Nesterov accelerated gradient
  - Adagrad
  - Adadelta
  - RMSprop
  - Adam

Network architecture types
  - Feed farward network
  - Recurrent neural network
  - Symetrically connected network

Unlabeled/unsupervised/feature extraction/pattern recognition
  - boltzman machine
  - Restricted boltzman machine
  - Autoencoders

Labeled/supervised/classifier
  - Recurrent network
  - Recursive neural tenser network
  - Convolutional network
  - Deep belief network


Steepest gradient Decent

Reinforcement learning?

Vectorization of layers?, neurons?, synapses?, weights and values
